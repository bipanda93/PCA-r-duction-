# ğŸŒ¸ PCA (Principal Component Analysis) - Iris Dataset

## ğŸ¯ Contexte et Objectif

### ProblÃ©matique
L'Analyse en Composantes Principales (ACP/PCA) est une technique de rÃ©duction de dimensionnalitÃ© essentielle en Data Science. Face Ã  des datasets avec de nombreuses variables corrÃ©lÃ©es, la PCA permet de :
- RÃ©duire la complexitÃ© des donnÃ©es (de 4 dimensions Ã  2)
- Visualiser des donnÃ©es multidimensionnelles
- Ã‰liminer la redondance entre variables
- Conserver un maximum d'information avec un minimum de dimensions

### Objectifs du projet
- Appliquer la PCA sur le cÃ©lÃ¨bre dataset Iris (4 variables â†’ 2 dimensions)
- InterprÃ©ter les valeurs propres et la variance expliquÃ©e
- Visualiser le cercle des corrÃ©lations (contribution des variables)
- Projeter les observations sur le plan factoriel
- Identifier la sÃ©parabilitÃ© des espÃ¨ces d'iris

## ğŸ—ï¸ Architecture technique

### Stack technique
- **Langage** : Python 3.x
- **Data Science** : NumPy, Pandas
- **Machine Learning** : scikit-learn (PCA, datasets, preprocessing)
- **Visualisation** : Matplotlib, Seaborn

### Pipeline d'analyse

```
1. IMPORTATION DES DONNÃ‰ES
   â””â”€ Dataset Iris (scikit-learn)

2. PRÃ‰PARATION
   â”œâ”€ Suppression de la variable cible (target)
   â””â”€ Standardisation (scaling)

3. APPLICATION DE LA PCA
   â”œâ”€ Fit sur donnÃ©es standardisÃ©es
   â””â”€ Extraction de 4 composantes principales

4. ANALYSE DES RÃ‰SULTATS
   â”œâ”€ Valeurs propres (eigenvalues)
   â”œâ”€ Variance expliquÃ©e par composante
   â””â”€ Variance cumulÃ©e

5. VISUALISATIONS
   â”œâ”€ Cercle des corrÃ©lations (contribution des variables)
   â””â”€ Plan factoriel (projection des observations)
```

## ğŸ“Š Dataset Iris

### Description
Le dataset Iris (Fisher, 1936) est un classique du Machine Learning :
- **150 observations** : 50 fleurs de chaque espÃ¨ce
- **3 espÃ¨ces** : Setosa, Versicolor, Virginica
- **4 variables numÃ©riques** (en cm) :

| Variable | Description | Type |
|----------|-------------|------|
| `sepal length` | Longueur du sÃ©pale | Float |
| `sepal width` | Largeur du sÃ©pale | Float |
| `petal length` | Longueur du pÃ©tale | Float |
| `petal width` | Largeur du pÃ©tale | Float |
| `target` | EspÃ¨ce (0/1/2) | Integer |

### PrÃ©paration des donnÃ©es

```python
from sklearn import datasets

# Importation
iris = datasets.load_iris(as_frame=True)
df = iris.frame

# Suppression de la variable cible
iris2 = df.drop("target", axis=1)
```

**Justification** : La PCA est une mÃ©thode **non supervisÃ©e** â†’ on retire la variable cible pour analyser uniquement la structure des features.

## ğŸ” Ã‰tapes de l'analyse PCA

### 1. Standardisation des donnÃ©es

```python
from sklearn.preprocessing import scale

# PCA nÃ©cessite des donnÃ©es standardisÃ©es (moyenne=0, Ã©cart-type=1)
iris_scaled = scale(iris2)
```

**Pourquoi standardiser ?**
- Les variables ont des unitÃ©s/Ã©chelles diffÃ©rentes
- Sans standardisation, les variables Ã  grande variance domineraient l'ACP
- La PCA est sensible aux Ã©chelles

---

### 2. Application de la PCA

```python
from sklearn.decomposition import PCA

# PCA avec 4 composantes (maximum possible avec 4 variables)
pca = PCA(n_components=4)
pca.fit(scale(iris2))
```

**ParamÃ¨tres** :
- `n_components=4` : Nombre de composantes principales Ã  extraire (ici, toutes)
- Alternative : `n_components=0.95` â†’ garde assez de composantes pour expliquer 95% de la variance

---

### 3. Analyse des valeurs propres

```python
eig = pd.DataFrame({
    "Dimension": ["Dim" + str(x + 1) for x in range(4)],
    "valeur propre": pca.explained_variance_,
    "% variance expliquÃ©e": np.round(pca.explained_variance_ratio_ * 100),
    "%cum.var.expliquÃ©e": np.round(np.cumsum(pca.explained_variance_ratio_)*100),
})
```

**RÃ©sultats typiques** :

| Dimension | Valeur propre | % variance | % cum. variance |
|-----------|---------------|------------|-----------------|
| Dim1 | 2.91 | 73% | 73% |
| Dim2 | 0.91 | 23% | 96% |
| Dim3 | 0.15 | 4% | 99% |
| Dim4 | 0.02 | 1% | 100% |

**InterprÃ©tation** :
- **Dim1 + Dim2** capturent **96% de la variance** â†’ 2 dimensions suffisent !
- **RÃ©duction de dimensionnalitÃ©** : 4D â†’ 2D avec seulement 4% de perte d'information
- Les dimensions 3 et 4 sont nÃ©gligeables (bruit)

**RÃ¨gle de Kaiser** : Conserver les dimensions avec valeur propre > 1 â†’ ici, Dim1 et Dim2

---

### 4. Projection des observations

```python
# Transformation des donnÃ©es originales dans le nouvel espace
iris_pca = pca.transform(scale(iris2))

# CrÃ©ation d'un DataFrame pour visualisation
iris_pca_df = pd.DataFrame({
    "Dim1": iris_pca[:,0],
    "Dim2": iris_pca[:,1],
    "Species": df["target"].map(dict(enumerate(iris.target_names)))
})
```

**RÃ©sultat** : Chaque observation (fleur) est maintenant reprÃ©sentÃ©e par 2 coordonnÃ©es (Dim1, Dim2) au lieu de 4.

---

### 5. CoordonnÃ©es des variables (Cercle des corrÃ©lations)

```python
# Calcul des coordonnÃ©es des variables sur les composantes
coordvar = pca.components_.T * np.sqrt(pca.explained_variance_)

coordvar_df = pd.DataFrame(
    coordvar,
    columns=['PC' + str(i) for i in range(1, 5)],
    index=iris2.columns
)
```

**Signification** :
- Ces coordonnÃ©es indiquent la **contribution** de chaque variable Ã  chaque composante
- Plus la coordonnÃ©e est proche de Â±1, plus la variable est importante pour cette dimension
- Permet d'interprÃ©ter le sens des axes factoriels

---

## ğŸ“ˆ Visualisations et InterprÃ©tations

### 1. Cercle des corrÃ©lations

```python
fig, axes = plt.subplots(figsize=(5,5))
axes.set_xlim(-1,1)
axes.set_ylim(-1,1)

# Axes de rÃ©fÃ©rence
axes.axvline(x=0, color='lightblue', linestyle='--')
axes.axhline(y=0, color='lightblue', linestyle='--')

# Projection des variables
for j in range(4):
    axes.text(coordvar_df["PC1"][j], coordvar_df["PC2"][j], 
              coordvar_df.index[j], size=12)
    axes.plot([0, coordvar_df["PC1"][j]], 
              [0, coordvar_df["PC2"][j]], 
              color="blue", linestyle='dashed')

# Cercle unitÃ©
axes.add_artist(plt.Circle((0,0), 1, color='red', fill=False))
plt.title("Cercle des corrÃ©lations")
plt.show()
```

**InterprÃ©tation** :

ğŸ“Š **RÃ¨gles de lecture** :
- **Longueur de la flÃ¨che** : Plus elle est longue (proche du cercle), mieux la variable est reprÃ©sentÃ©e sur ce plan
- **Angle entre flÃ¨ches** :
  - Proche de 0Â° â†’ Variables fortement corrÃ©lÃ©es positivement
  - Proche de 90Â° â†’ Variables indÃ©pendantes
  - Proche de 180Â° â†’ Variables corrÃ©lÃ©es nÃ©gativement
- **Position sur l'axe** : Indique quelle dimension capture quelle variable

ğŸŒ¸ **RÃ©sultats typiques Iris** :
- `petal length` et `petal width` : TrÃ¨s corrÃ©lÃ©es (flÃ¨ches parallÃ¨les)
- Ces deux variables contribuent fortement Ã  **Dim1** (axe horizontal)
- `sepal width` : Contribue davantage Ã  **Dim2** (axe vertical)
- **Dim1** = "Dimension de la taille globale de la fleur"
- **Dim2** = "Dimension de la forme (ratio longueur/largeur)"

---

### 2. Plan factoriel (Projection des observations)

```python
g_pca = sn.lmplot(x="Dim1", y="Dim2", hue="Species", 
                  data=iris_pca_df, fit_reg=False, 
                  height=4, aspect=3)
g_pca.set(xlabel="Dimension 1 (73%)", ylabel="Dimension 2 (23%)")
g_pca.fig.suptitle("Premier plan Factoriel")
plt.show()
```

**InterprÃ©tation** :

ğŸ¨ **SÃ©parabilitÃ© des espÃ¨ces** :
- **Setosa** : Cluster bien distinct, complÃ¨tement sÃ©parÃ© (en bas Ã  gauche gÃ©nÃ©ralement)
- **Versicolor** et **Virginica** : Partiellement superposÃ©es (centre-droit)
- **Dim1** (73%) : Principale source de sÃ©paration
- **Dim2** (23%) : Affine la sÃ©paration entre Versicolor et Virginica

ğŸ’¡ **Insights** :
- La PCA rÃ©vÃ¨le une structure naturelle dans les donnÃ©es
- 2 dimensions suffisent pour visualiser 96% de l'information
- Facilite les algorithmes de classification (K-means, SVM, etc.)

---

## ğŸ“š CompÃ©tences dÃ©montrÃ©es

### Pour les recruteurs Data Scientist / ML Engineer

**1. RÃ©duction de dimensionnalitÃ©**
- ComprÃ©hension thÃ©orique de la PCA
- Application pratique avec scikit-learn
- InterprÃ©tation des valeurs propres et variance expliquÃ©e

**2. Preprocessing**
- Standardisation des donnÃ©es (`scale`)
- PrÃ©paration pour algorithmes ML non supervisÃ©s

**3. Analyse statistique**
- InterprÃ©tation du cercle des corrÃ©lations
- Analyse de la structure des donnÃ©es multidimensionnelles
- Identification de la redondance entre variables

**4. Visualisation avancÃ©e**
- Cercle des corrÃ©lations (matplotlib personnalisÃ©)
- Plan factoriel avec seaborn
- Communication visuelle de rÃ©sultats complexes

**5. Applications mÃ©tier**
- **Feature engineering** : RÃ©duction de features avant modÃ©lisation
- **Data exploration** : Comprendre les relations entre variables
- **Visualisation** : ReprÃ©senter des donnÃ©es haute dimension
- **Compression** : RÃ©duire la complexitÃ© sans perdre d'information

## ğŸ”§ Applications concrÃ¨tes de la PCA

### 1. Machine Learning
```python
# RÃ©duire les features avant classification
pca = PCA(n_components=0.95)  # Garde 95% de variance
X_reduced = pca.fit_transform(X_train)

# EntraÃ®ner un modÃ¨le sur donnÃ©es rÃ©duites (plus rapide)
model.fit(X_reduced, y_train)
```

**Avantages** :
- RÃ©duction du temps d'entraÃ®nement
- Moins de risque d'overfitting
- Gestion de la multicollinÃ©aritÃ©

---

### 2. Compression d'images

```python
# Image = matrice de pixels â†’ appliquer PCA
pca = PCA(n_components=50)  # Garde 50 composantes sur 1000 pixels
image_compressed = pca.fit_transform(image)

# Reconstruction avec perte minimale
image_reconstructed = pca.inverse_transform(image_compressed)
```

**RÃ©sultat** : Compression de 95% avec qualitÃ© visuelle prÃ©servÃ©e

---

### 3. DÃ©tection d'anomalies

```python
# Projeter sur 2 composantes
X_pca = pca.fit_transform(X)

# Les points Ã©loignÃ©s du centre = anomalies
distances = np.linalg.norm(X_pca, axis=1)
anomalies = X[distances > threshold]
```

---

### 4. Analyse de sentiment (NLP)

```python
# 10,000 mots â†’ 100 dimensions PCA
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=10000)
X_tfidf = tfidf.fit_transform(texts)

pca = PCA(n_components=100)
X_reduced = pca.fit_transform(X_tfidf.toarray())
```

**UtilitÃ©** : Visualiser les clusters de documents similaires

---

## ğŸ“‹ Concepts thÃ©oriques clÃ©s

### MathÃ©matiques de la PCA

**1. Objectif** : Trouver les axes orthogonaux qui maximisent la variance des donnÃ©es projetÃ©es

**2. Ã‰tapes mathÃ©matiques** :
```
1. Centrer les donnÃ©es (moyenne = 0)
2. Calculer la matrice de covariance
3. Calculer les vecteurs propres (directions) et valeurs propres (importance)
4. Trier par valeurs propres dÃ©croissantes
5. Projeter les donnÃ©es sur les k premiers vecteurs propres
```

**3. PropriÃ©tÃ©s** :
- Les composantes principales sont **orthogonales** (non corrÃ©lÃ©es)
- La premiÃ¨re composante capture le **maximum de variance**
- La PCA est une transformation **linÃ©aire** (limitation)

---

### Quand utiliser la PCA ?

âœ… **Cas d'usage appropriÃ©s** :
- Variables numÃ©riques continues
- Variables corrÃ©lÃ©es entre elles
- Besoin de visualisation (haute dimension â†’ 2D/3D)
- RÃ©duction de features avant modÃ©lisation
- DonnÃ©es bruitÃ©es (la PCA filtre le bruit)

âŒ **Limitations** :
- **Perte d'interprÃ©tabilitÃ©** : Les composantes sont des combinaisons linÃ©aires difficiles Ã  nommer
- **LinÃ©aritÃ©** : Ne capture pas les relations non-linÃ©aires (alternative : t-SNE, UMAP)
- **Sensible aux outliers** : Les valeurs extrÃªmes influencent les axes
- **Pas adaptÃ© aux variables catÃ©gorielles** : NÃ©cessite des donnÃ©es numÃ©riques

---

## ğŸ”§ Reproduction du projet

### PrÃ©requis

```bash
pip install pandas numpy matplotlib seaborn scikit-learn
```

### ExÃ©cution

```python
# Lancer le script
python pca_iris_analysis.py
```

### Structure du projet

```
pca-iris-analysis/
â”œâ”€â”€ pca_iris_analysis.py        # Script principal
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ correlation_circle.png  # Cercle des corrÃ©lations
â”‚   â””â”€â”€ factorial_plan.png      # Plan factoriel
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“– Contexte

**RÃ©alisÃ© dans le cadre** : Formation personnelle  
**Objectif** : Approfondir les compÃ©tences en Machine Learning non supervisÃ©  
**DurÃ©e** : Auto-formation  
**Focus** : MaÃ®triser la rÃ©duction de dimensionnalitÃ© et l'interprÃ©tation de la PCA

---

ğŸ“§ Contact

Franck Ulrich BIPANDA 

ğŸ“§ bipanda.franck@icloud.com  
ğŸ”— [LinkedIn](https://linkedin.com/in/franck-bipanda-13392372)  
ğŸŒ [Portfolio](https://datascienceportfol.io/bipandaf)
